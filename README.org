A simple Q learning implementation learning how to play Pok√©mon Red.

Work in progress.

* Generate Training Data

Run

#+begin_src 
python main.py --processes 0 --headless
#+end_src

to run headless, with processes = num cpus, and saving replay data (see env.replay_buffer) to the replays folder.

See =main.py= for other CLI arguments.


* Train (and run) a Q model

#+begin_src
python main.py --train_from_replays
#+end_src

Trains from the replay of all replays in the replay folder, sampled randomly. See =ai_agent.train_from_replays=.
It will then print out the filename containing the Q table of the model.
Finally, it will run the agent with the Q table and an exploration rate of 20%

* Running

Running train_from_replays will generate a pkl file for the resulting Q state. Load that file to have the AI agent play:

#+begin_src 
python main.py --initial_q_state <q_state_file>
#+end_src

** Manual mode

#+begin_src
python main.py --manual
#+end_src


* Visualizing results

=viz_rewards.py= will visualize the rewards on each tile integrated over all replays in the replays folder

=utils.py= can be used to quickly see summary stats, and can also visually debug individual episodes  with the --viz option
