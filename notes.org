* Comparison to pokemonred_puffer

They don't use map x/y in obs (agent observation?) They use screen render and map_id (and blackout_map_id?)
They don't seem to store history of location/screens/inputs in the observation either, so how does it navigate?

** visited_mask is same size as render field. what is it

#+begin_src  python
visited_mask = (
    255
    ,* np.repeat(
        np.repeat(self.explore_map[gr - 4 : gr + 6, gc - 4 : gc + 6], 16 // scale, 0),
        16 // scale,
        -1,
    )
).astype(np.uint8)[6 // scale : -10 // scale, :]

...

self.explore_map = np.zeros(GLOBAL_MAP_SHAPE, dtype=np.float32)
#+end_src

#+begin_src python
def update_seen_coords(self):
    inc = 0.5 if (self.read_m("wMovementFlags") & 0b1000_0000) else self.exploration_inc

    x_pos, y_pos, map_n = self.get_game_coords()
    # self.seen_coords[(x_pos, y_pos, map_n)] = inc
    cur_map_tileset = self.read_m("wCurMapTileset")
    if cur_map_tileset not in self.seen_coords:
        self.seen_coords[cur_map_tileset] = {}
        self.seen_coords[cur_map_tileset][(x_pos, y_pos, map_n)] = min(
            self.seen_coords[cur_map_tileset].get((x_pos, y_pos, map_n), 0.0) + inc,
            self.exploration_max,
        )
        # TODO: Turn into a wrapper?
    self.explore_map[local_to_global(y_pos, x_pos, map_n)] = min(
        self.explore_map[local_to_global(y_pos, x_pos, map_n)] + inc,
        self.exploration_max,
    )
    self.reward_explore_map[local_to_global(y_pos, x_pos, map_n)] = min(
        self.explore_map[local_to_global(y_pos, x_pos, map_n)] + inc,
        self.exploration_max,
    ) * (self.map_id_scalefactor if self.scale_map_id(map_n) else 1.0)
    # self.seen_global_coords[local_to_global(y_pos, x_pos, map_n)] = 1
    self.seen_map_ids[map_n] = 1
#+end_src

#+begin_src python baseline.py
class BaselineRewardEnv(RedGymEnv):
    def get_game_state_reward(self):
            "explore": sum(sum(tileset.values()) for tileset in self.seen_coords.values()) * 0.012,
#+end_src

* TODO Save checkpoints on positions with best cumulative rewards, and start future envs/agents from that point

How do we choose what to save/run?
Keep provenance of "timelines"? To avoid going down a rabbit hole without sonide

Choose runs that got to 90% of cumulative goal in < 20% steps, then continue from those checkpoints,  with many branches? Can choose stochastically wrt some measure of f(cumulative goal percentile, steps_to_reach)

The advantage of the above, is we have a way to generate better episodes without modifying episode length, which may help training?

* TODO https://docs.pola.rs/user-guide/io/parquet/
* TODO Fix cumulative reward training for exploration

** TODO Rescale cumulative rewards retroactively

I.e., now we have:

#+begin_src bash
Number of episodes: 1000
Min reward: 721.00
Max reward: 1374.00
Mean reward: 976.41
Median reward: 978.50
Std dev: 82.69
25th percentile: 919.00
75th percentile: 1036.00
90th percentile: 1080.00
#+end_src


The 2x reward diff for the worse vs best episode is bad training. The reward signal is not clear enough. We should bias towards top 5% (TODO? hyperparam) of results, other cumulative rewards should be very low. 0?

Also, reward is very low even at max? Need to start using policy during training to do better than random.

Note: ~2200 nearly visited coords (cumulative reward) in the area before you pass the sleepy man.
Including trying to go to elite 4, not including all buildings 

cumulative reward 3337 after visiting like whole map without pokedex

*** 60 file replay with < 1080 cumulative rewards clamped to 0:

checkpoints/from_replays/agent_state_20241110_192541_XDcLS5HZ.pkl

Performance seems ok? 

*** 1000 file replay, same rewards as above
checkpoints/from_replays/agent_state_20241110_192800_dOzEJYF6.pkl

Struggles in battles, need to fix menu state probably

* TODO Use policy with non-1.0 exploration during training

Exploration does okay with reward scaling, training on random inputs. After adding episodes with trained input, it does worse

* TODO Go through pokemonred_puffer read_m calls, note important addrs
like 
#+begin_src python
if self.read_m("wCurrentMenuItem") == 2:
#+end_src

Note: lookup of this symbol fails, but we can read from the hex addr

* TODO Fix battle state

May be fixed, need to test

Regenerated training data with new menu state. Now to train! And generate more data with non-1.0

checkpoints/from_replays/agent_state_20241111_142424_9v2z4GfV.pkl

* TODO Eval models performance (in terms of cumulative reward) after training / on-demand

* TODO  better sampling

* TODO Find optimal hold_ticks and release_ticks
* TODO Set up training loop where we get top 10% performance on cumulative_reward, train with other runs clamped, then generate new training data with the model at exploration rate = ??
* TODO Hindsight experience replay?
* TODO "Swarming" ie good savepoints to explore from instead of starting tile

Probably should do checkpoints when training and compare performance. Don't want to overtrain right ü§î?

* Ideas

** Rewrite it in Rust

Not just a joke, it could actually make training a lot faster, without needing a GPU. (Assuming we stick with Q learning for a while. May also make some things faster even after going GPU though)

Using Polars with Parquet may be a reasonable middleground?

* gipity archives
:PROPERTIES:
:VISIBILITY: folded
:END:


** Write a rust program to ingest all the replay pkl files, save them to a datafusion database, and query statistics on them
Here's a concise Rust program that handles replay pkl files using DataFusion:

#+begin_src rust
use datafusion::prelude::*;
use serde::Deserialize;
use std::fs;
use std::path::Path;

#[derive(Deserialize)]
struct Replay {
    // Define your replay structure here
    timestamp: i64,
    player_name: String,
    // Add other fields
}

async fn process_replays() -> Result<(), Box<dyn std::error::Error>> {
    let ctx = SessionContext::new();
    
    // Create table schema
    let schema = Schema::new(vec![
        Field::new("timestamp", DataType::Int64, false),
        Field::new("player_name", DataType::Utf8, false),
        // Add other fields
    ]);

    // Process all pkl files
    let paths = fs::read_dir("./replays")?;
    let mut data = Vec::new();
    
    for path in paths {
        let file_path = path?.path();
        if file_path.extension().unwrap_or_default() == "pkl" {
            let bytes = fs::read(&file_path)?;
            let replay: Replay = serde_pickle::from_slice(&bytes, Default::default())?;
            data.push(vec![
                ScalarValue::Int64(Some(replay.timestamp)),
                ScalarValue::Utf8(Some(replay.player_name)),
                // Add other fields
            ]);
        }
    }

    // Create DataFrame
    let df = ctx.create_dataframe(vec![
        Arc::new(MemTable::try_new(schema, vec![data])?),
    ])?;

    // Example queries
    df.filter(col("timestamp").gt(lit(1000)))?
        .select(vec![col("player_name")])?
        .show()
        .await?;

    Ok(())
}

fn main() {
    tokio::runtime::Runtime::new()
        .unwrap()
        .block_on(process_replays())
        .unwrap();
}
#+end_src

Add necessary dependencies to Cargo.toml:

#+begin_src toml
[dependencies]
datafusion = "8.0"
serde = { version = "1.0", features = ["derive"] }
serde_pickle = "1.1"
tokio = { version = "1.0", features = ["full"] }
#+end_src

Modify the =Replay= struct fields according to your pickle file structure.

** exploration strategies
:PROPERTIES:
:GPTEL_MODEL: claude-3-5-haiku-20241022
:GPTEL_BACKEND: Claude
:GPTEL_SYSTEM: You are a large language model living in Emacs and a helpful assistant. Respond concisely.
:GPTEL_BOUNDS: ((227 . 339))
:END:

*** hi

Hello! How can I help you today? Would you like assistance with your Pok√©mon Red reinforcement learning project?

*** yes please help it doesnt explore well

Based on the code, a few potential improvements for exploration:

1. Gradually decrease =exploration_rate= over time (epsilon decay)
2. Use softmax action selection instead of epsilon-greedy
3. Implement more sophisticated exploration strategies like:
   - UCB (Upper Confidence Bound)
   - Thompson Sampling
   - Boltzmann exploration

Would you like me to sketch out an implementation of one of these approaches?

*** Explain those exploration strategies

Here's a concise overview:

1. Epsilon Decay:
- Start with high exploration (e.g., Œµ = 1.0)
- Gradually reduce Œµ as training progresses
- Eventually converges to mostly exploitation
- Simple, but can be too aggressive

2. Softmax (Boltzmann) Exploration:
- Choose actions probabilistically based on Q-values
- Uses temperature parameter to control exploration
- High temperature = more random
- Low temperature = more greedy
- Smoother transition between exploration and exploitation

3. Upper Confidence Bound (UCB):
- Balances exploration by tracking action uncertainty
- Calculates a confidence interval for each action
- Prioritizes actions with high potential/uncertainty
- More mathematically principled exploration

4. Thompson Sampling:
- Maintains a probability distribution over action values
- Samples an action based on its estimated probability of being optimal
- Naturally balances exploration and exploitation
- Bayesian approach, slightly more complex to implement

Each has pros and cons. Softmax or UCB might work well for your game environment.

*** Whatsthe difference between boltzmann and Thompson sampling

Great question! Key differences:

Boltzmann (Softmax):
- Probabilistic selection based on Q-values
- Uses a temperature parameter to control randomness
- Directly uses current Q-value estimates
- Simpler to implement
- Deterministic exploration strategy

Thompson Sampling:
- Maintains full probability distribution over action values
- Samples from a Bayesian posterior distribution
- Accounts for uncertainty in value estimates
- More adaptive to learning
- Probabilistic Bayesian approach
- Handles exploration more dynamically
- Slightly more complex mathematically

In practice:
- Boltzmann: Good for smooth, controlled exploration
- Thompson: Better for handling uncertainty and adapting exploration

For your game, Boltzmann might be easier to start with.

*** For UCB, how is action uncertainty determined?

In UCB (Upper Confidence Bound), action uncertainty is determined by two key components:

1. Exploitation Term:
- Average reward for that action
- Higher average reward = higher preference

2. Exploration Term:
- Calculated as: ‚àö(log(total_steps) / times_action_selected)
- Increases for actions selected less frequently
- Ensures less-explored actions get more chances

UCB Formula:
Q(a) + c * ‚àö(log(N) / N(a))

Where:
- Q(a): Average reward for action
- N: Total number of steps
- N(a): Number of times action was selected
- c: Exploration parameter (typically ‚àö2)

Actions with:
- High uncertainty
- Few selections
- Potential for high reward

Get higher selection probability.

This mathematically balances exploration of unknown actions with exploitation of known good actions.

*** 

** exploration - potentialbasedrewardshaping

**** good_cumulative_boosts_breadcumbs, give feedback on that part of the code

Looking at the reward calculation in =train_from_replays= in =ai_agent.py=:

#+begin_src python
if (
    good_cumulative_boosts_breadcumbs
    and episode_cumulative_reward >= cumulative_reward_90p
    and reward > 0
):
    reward = reward * 200
else:
    reward = 0
#+end_src

This approach has some interesting aspects:

Pros:
- Focuses on experiences from episodes that performed well (above 90th percentile)
- Creates a form of curriculum learning by amplifying rewards from successful trajectories
- The multiplier (200x) provides strong signal for good behaviors

Potential issues:
- Setting non-amplified rewards to 0 might be too extreme - could lose useful learning signal from decent but not exceptional episodes
- The 90th percentile threshold is quite aggressive - might discard too many useful experiences 
- The 200x multiplier is arbitrary and may need tuning

Suggestions:
1. Consider a graduated reward scaling rather than binary (0 or 200x)
2. Could use multiple percentile tiers (e.g. 75th = 50x, 90th = 100x, 95th = 200x)
3. Keep some base reward even for non-exceptional episodes
4. Add comments explaining the reasoning behind the specific values chosen

Would you like me to elaborate on any of these points?

**** Please elaborate on all points

Let's dive deep into each point:

1. Graduated Reward Scaling:
#+begin_src python
# Current approach (binary):
reward = reward * 200 if good_episode else 0

# Graduated approach example:
percentile_rank = get_percentile_rank(episode_cumulative_reward, cumulative_rewards)
scaling_factor = max(1.0, percentile_rank)  # Linear scaling based on percentile
reward = reward * scaling_factor
#+end_src

This provides smoother learning gradients and rewards incremental improvements.

2. Multiple Percentile Tiers:
#+begin_src python
def get_reward_scaling(episode_reward, reward_thresholds):
    if episode_reward >= reward_thresholds['95p']:
        return 200
    elif episode_reward >= reward_thresholds['90p']:
        return 100
    elif episode_reward >= reward_thresholds['75p']:
        return 50
    else:
        return 1

reward_thresholds = {
    '95p': np.percentile(cumulative_rewards, 95),
    '90p': np.percentile(cumulative_rewards, 90),
    '75p': np.percentile(cumulative_rewards, 75)
}

scaling = get_reward_scaling(episode_cumulative_reward, reward_thresholds)
reward = reward * scaling
#+end_src

This creates clearer progression tiers while still maintaining some reward signal for all experiences.

3. Base Reward Preservation:
#+begin_src python
# Current approach:
reward = reward * 200 if good_episode else 0

# Modified approach:
base_scaling = 1.0
bonus_scaling = 199.0  # Total max scaling still 200x
reward = (reward * base_scaling) + (reward * bonus_scaling if good_episode else 0)
#+end_src

This ensures some learning happens even from mediocre episodes while still heavily emphasizing good ones.

4. Documentation:
#+begin_src python
def calculate_reward_scaling(episode_reward, cumulative_rewards):
    """Calculate reward scaling factor based on episode performance.
    
    Args:
        episode_reward: The cumulative reward for this episode
        cumulative_rewards: Historical rewards for percentile calculation
        
    Returns:
        float: Reward scaling factor
    
    Design choices:
    - 90th percentile threshold identifies consistently good episodes
    - 200x multiplier chosen to:
      a) Overcome noise from random exploration
      b) Create strong enough gradient for Q-learning
      c) Based on empirical testing showing learning stability
    - Base reward preserved to maintain exploration
    """
    percentile_90 = np.percentile(cumulative_rewards, 90)
    base_scaling = 1.0
    bonus_scaling = 199.0
    
    if episode_reward >= percentile_90:
        return base_scaling + bonus_scaling
    return base_scaling
#+end_src

Additional Considerations:

1. Dynamic Scaling:
#+begin_src python
# Scale could adjust based on training progress
training_progress = sample_i / total_samples
max_scaling = 200 * (1 - training_progress)  # Gradually reduce scaling
#+end_src

2. Memory Prioritization:
#+begin_src python
# Could tie into prioritized experience replay
experience_priority = reward * scaling_factor
replay_buffer.add(experience, priority=experience_priority)
#+end_src

3. Validation:
#+begin_src python
# Add metrics to validate reward scaling effectiveness
if sample_i % 1000 == 0:
    avg_reward = np.mean([exp['reward'] for exp in recent_experiences])
    avg_scaled_reward = np.mean([exp['scaled_reward'] for exp in recent_experiences])
    print(f"Avg reward: {avg_reward:.2f}, Scaled: {avg_scaled_reward:.2f}")
#+end_src

The key is finding the right balance between:
- Emphasizing good behaviors strongly enough to learn from them
- Not completely discarding less optimal but still informative experiences
- Maintaining enough exploration through base rewards
- Creating clear but achievable progression targets

Would you like me to elaborate on any specific aspect of these implementations?

**** Do any of these strategies have similarities to techniques found successfull in reinforcement learning papers

Yes, several of these strategies parallel established RL techniques from literature:

1. *Prioritized Experience Replay* (PER) - Schaul et al., 2015
#+begin_src python
# Our current approach of boosting rewards from good episodes is a simple form of:
reward = reward * 200 if episode_cumulative_reward >= cumulative_reward_90p else 0

# PER instead assigns priorities based on TD-error:
priority = |reward + gamma * max_Q(s') - Q(s,a)|
#+end_src
PER is generally more principled as it prioritizes surprising/informative experiences rather than just high-reward ones.

2. *Curriculum Learning* - Bengio et al., 2009
Our percentile-based approach resembles curriculum learning:
#+begin_src python
# Our tiered rewards:
if episode_reward >= reward_thresholds['95p']:
    scaling = 200
elif episode_reward >= reward_thresholds['90p']:
    scaling = 100
#+end_src
But established curriculum approaches usually structure the environment/tasks rather than just reward scaling.

3. *Hindsight Experience Replay* (HER) - Andrychowicz et al., 2017
While we boost rewards post-hoc, HER more formally reframes experiences with different goals:
#+begin_src python
# HER would store multiple versions of each trajectory with different goals
# Rather than our simpler reward boosting:
reward = reward * scaling_factor if good_episode else reward
#+end_src

4. *Reward Shaping* - Ng et al., 1999
Our approach is a form of reward shaping, but the classical theory suggests preserving optimal policies through potential functions:
#+begin_src python
# Proper reward shaping:
shaped_reward = reward + gamma * Phi(next_state) - Phi(state)

# vs our current approach:
shaped_reward = reward * scaling  # Could violate policy invariance
#+end_src

Would you like me to elaborate on any of these connections or discuss other relevant RL techniques?

**** Explain potential functions, implementation and theory / motivation

Potential-based reward shaping is a theoretically grounded way to modify rewards without changing the optimal policy. Here's a breakdown:

Theory:
#+begin_src 
F(s,a,s') = Œ≥Œ¶(s') - Œ¶(s)

where:
- F is the shaping reward added to environment reward
- Œ¶ is the potential function mapping states to real numbers 
- Œ≥ is the discount factor
- s,s' are current/next states
#+end_src

The key insight is that this form guarantees policy invariance - the optimal policy remains unchanged regardless of shaping.

Example Implementation:
#+begin_src python
class PotentialBasedRewardShaping:
    def __init__(self, gamma=0.99):
        self.gamma = gamma
    
    def potential(self, state: EnvironmentState) -> float:
        """Map state to potential value.
        
        Higher potential = closer to goal state.
        """
        # Example: Use distance to goal as potential
        target_pos = (309, 99)  # Oak's lab coordinates
        distance = np.sqrt(
            (state.position[0] - target_pos[0])**2 + 
            (state.position[1] - target_pos[1])**2
        )
        
        # Convert distance to potential (closer = higher potential)
        potential = 1000 / (1 + distance)
        
        # Could incorporate other state features
        if state.has_oaks_parcel:
            potential += 500
            
        return potential

    def shaped_reward(self, state, action, next_state, env_reward):
        """Add shaping reward to environment reward."""
        shaping = (
            self.gamma * self.potential(next_state) - 
            self.potential(state)
        )
        return env_reward + shaping
#+end_src

Usage in training loop:
#+begin_src python
shaper = PotentialBasedRewardShaping()

def train_from_replays(self):
    for experience in replay_buffer:
        state = experience["state"]
        next_state = experience["next_state"] 
        env_reward = experience["reward"]
        
        # Get shaped reward
        shaped_reward = shaper.shaped_reward(
            state, action, next_state, env_reward
        )
        
        # Update Q-values using shaped reward
        self.update_q_table(state, action, next_state, shaped_reward)
#+end_src

The motivation is to provide more immediate feedback while preserving optimal behavior. For example, in Pok√©mon:
- Raw reward might only come from getting Oak's Parcel
- But potential function gives higher values for being closer to Oak's lab
- Agent gets reward signal for moving in right direction
- Yet optimal policy (shortest path to parcel) remains unchanged

This is more principled than our current arbitrary reward scaling, though requires careful potential function design.

Would you like me to elaborate on potential function design strategies or show more implementation details?

***** How is it optimal

Let me explain why potential-based shaping preserves optimality:

1. *Mathematical Proof Sketch*
#+begin_src 
Let V*(s) be optimal value function for original rewards R(s,a,s')
Let V'(s) be value function with shaped rewards R'(s,a,s') where:
R'(s,a,s') = R(s,a,s') + Œ≥Œ¶(s') - Œ¶(s)

Key insight: V'(s) = V*(s) + Œ¶(s)

Proof:
- For any policy œÄ, shaped value function V'œÄ differs from original VœÄ by Œ¶(s)
- Therefore optimal policy under shaped rewards must be optimal under original rewards
#+end_src

2. *Concrete Example*:
#+begin_src python
# Consider two paths to Oak's lab:
# Path A: 10 steps
# Path B: 20 steps

class SimpleExample:
    def potential(self, steps_to_goal):
        return 100 / (1 + steps_to_goal)
    
    def demonstrate_paths(self):
        # Path A (optimal)
        state_A = 10  # steps remaining
        next_state_A = 9
        
        # Original reward (sparse)
        orig_reward_A = 0  # no reward until goal
        
        # Shaped reward
        shaping_A = (
            0.99 * self.potential(9) -  # Œ≥Œ¶(s') 
            self.potential(10)          # Œ¶(s)
        )
        shaped_reward_A = orig_reward_A + shaping_A
        
        # Path B (suboptimal) 
        state_B = 20
        next_state_B = 19
        
        # Same calculation for longer path
        shaping_B = (
            0.99 * self.potential(19) -
            self.potential(20)
        )
        shaped_reward_B = orig_reward_B + shaping_B
        
        # Key: While both get immediate rewards,
        # Path A's accumulated shaped reward will still be higher
#+end_src

3. *Why It Preserves Optimality*:
#+begin_src python
def explain_optimality():
    """
    1. The shaping term Œ≥Œ¶(s') - Œ¶(s) forms a telescoping series
    
    2. For any trajectory s0,s1,...,sn:
       Sum of shaping = Œ≥Œ¶(s1) - Œ¶(s0) +
                       Œ≥Œ¶(s2) - Œ¶(s1) +
                       ... +
                       Œ≥Œ¶(sn) - Œ¶(sn-1)
                     = -Œ¶(s0) + Œ≥^nŒ¶(sn)
    
    3. This means total shaped reward only depends on:
       - Start state (same for all trajectories)
       - Final state (same reward for reaching goal)
       - Path length (through Œ≥^n term)
       
    4. Therefore shorter paths to goal remain optimal
    """
#+end_src

4. *Practical Implementation*:
#+begin_src python
class PokemonShaping:
    def __init__(self, gamma=0.99):
        self.gamma = gamma
        self.oak_lab_pos = (309, 99)
        
    def potential(self, state: EnvironmentState) -> float:
        # Distance component
        dist = manhattan_distance(state.position, self.oak_lab_pos)
        distance_potential = 1000 / (1 + dist)
        
        # Progress component
        progress_potential = 0
        if state.has_oaks_parcel:
            progress_potential += 500
            
        return distance_potential + progress_potential
        
    def shaped_reward(self, state, next_state, env_reward):
        shaping = (
            self.gamma * self.potential(next_state) -
            self.potential(state)
        )
        return env_reward + shaping

    def update_agent(self, state, action, next_state, env_reward):
        shaped_reward = self.shaped_reward(state, next_state, env_reward)
        # Even

*** I dont want to define reward in terms of location relative to a specific location, like oaks lab. I want to reward for exploration more generally. can I still use this potential approach

Yes! For exploration-based potential functions, we can use concepts like state visitation counts or coverage of the map. Here's how:

#+begin_src python
class ExplorationPotential:
    def __init__(self, gamma=0.99, vicinity_size=2):
        self.gamma = gamma
        self.vicinity_size = vicinity_size
        # Track visited areas with a set of coordinates
        self.visited_coords = set()
        self.nearly_visited_coords = set()
        
    def add_vicinity_to_nearly_visited(self, position):
        """Add 5x5 area around position to nearly_visited"""
        for dy in range(-self.vicinity_size, self.vicinity_size + 1):
            for dx in range(-self.vicinity_size, self.vicinity_size + 1):
                nearby = (position[0] + dy, position[1] + dx)
                self.nearly_visited_coords.add(nearby)

    def potential(self, state: EnvironmentState) -> float:
        """Potential increases with exploration coverage"""
        # Add current position and vicinity to visited sets
        self.visited_coords.add(state.position)
        self.add_vicinity_to_nearly_visited(state.position)
        
        # Potential based on total unique areas discovered
        return len(self.nearly_visited_coords)

    def shaped_reward(self, state, next_state, env_reward):
        """Reward shaping using the potential difference"""
        shaping = (
            self.gamma * self.potential(next_state) -
            self.potential(state)
        )
        return env_reward + shaping
#+end_src

This maintains the theoretical guarantees while rewarding exploration because:
1. Higher potential for states that have led to more exploration
2. The shaping term rewards transitions that increase coverage
3. Still forms a telescoping series, preserving optimality for paths that maximize exploration

We could also add other exploration metrics to the potential:

#+begin_src python
class SophisticatedExplorationPotential:
    def __init__(self, gamma=0.99):
        self.gamma = gamma
        self.visited_coords = set()
        self.visit_counts = defaultdict(int)
        self.edge_counts = defaultdict(int)
        
    def potential(self, state: EnvironmentState) -> float:
        """Multiple components in exploration potential"""
        # 1. Coverage potential
        coverage = len(self.visited_coords)
        
        # 2. Novelty potential (reward visiting less frequent areas)
        pos = state.position
        novelty = 1.0 / (1.0 + self.visit_counts[pos])
        
        # 3. Frontier potential (reward being near unexplored areas)
        frontier_bonus = self._calculate_frontier_bonus(pos)
        
        # Update tracking
        self.visited_coords.add(pos)
        self.visit_counts[pos] += 1
        
        return (
            coverage * 1.0 +      # Base exploration term
            novelty * 50.0 +      # Encourage visiting new areas
            frontier_bonus * 25.0  # Encourage frontier exploration
        )
    
    def _calculate_frontier_bonus(self, pos):
        """Higher bonus for positions adjacent to unexplored areas"""
        adjacent = self._get_adjacent_positions(pos)
        unexplored = sum(1 for adj in adjacent if adj not in self.visited_coords)
        return unexplored / len(adjacent)
#+end_src

This approach rewards:
- Discovering new areas
- Revisiting less frequent locations
- Exploring edges of known territory

Would you like me to elaborate on any of these approaches or show how to integrate them with the existing replay buffer training?

**** Show how to integrate 

Here's how to integrate the exploration-based potential shaping with the replay training:

#+begin_src python
class AIAgent:
    def __init__(self, learning_rate=0.05, discount_factor=0.9, exploration_rate=0.7):
        self.q_table = defaultdict(lambda: np.zeros(len(Actions.list())))
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate
        # Add potential shaping
        self.shaper = ExplorationPotential(gamma=discount_factor)

    def train_from_replays(self, agent_id=None, replays_dir="replays"):
        """Train agent using stored replay experiences with potential-based shaping"""
        all_experiences = []
        
        # Load experiences (existing code)
        print("Loading replay files")
        for filename in tqdm.tqdm(list(os.listdir(replays_dir))[:1000]):
            if filename.endswith(".pkl"):
                replay_buffer = ReplayBuffer()
                replay_buffer.load(os.path.join(replays_dir, filename))
                all_experiences.extend(replay_buffer.buffer)

        print(f"Collected {len(all_experiences)} experiences")
        
        # Group experiences by episode to maintain exploration tracking
        episodes = self._group_experiences_by_episode(all_experiences)
        
        print("Training from episodes")
        for episode_idx, episode in enumerate(tqdm.tqdm(episodes)):
            # Reset exploration tracking for each episode
            self.shaper = ExplorationPotential(gamma=self.discount_factor)
            
            # Process each step in episode
            for experience in episode:
                state = experience["state"]
                action = experience["action"]
                next_state = experience["next_state"]
                env_reward = experience["reward"]
                
                # Get shaped reward using potential difference
                shaped_reward = self.shaper.shaped_reward(
                    state, 
                    next_state,
                    env_reward
                )
                
                # Update Q-table with shaped reward
                self._update_q_value(state, action, next_state, shaped_reward)

                # Save checkpoint periodically
                if agent_id and episode_idx % 100 == 0:
                    self.save_state(f"checkpoints/{agent_id}_episode_{episode_idx}.pkl")

    def _group_experiences_by_episode(self, experiences):
        """Group experiences into episodes based on done flag"""
        episodes = []
        current_episode = []
        
        for exp in experiences:
            current_episode.append(exp)
            if exp["done"]:
                episodes.append(current_episode)
                current_episode = []
        
        # Add final episode if not empty
        if current_episode:
            episodes.append(current_episode)
            
        return episodes

    def _update_q_value(self, state, action, next_state, shaped_reward):
        """Update Q-table using Q-learning update rule"""
        action_idx = Actions.list().index(action)
        
        # Get max Q-value for next state
        next_max_q = np.max(self.q_table[next_state])
        
        # Current Q-value
        current_q = self.q_table[state][action_idx]
        
        # Q-learning update
        new_q = current_q + self.learning_rate * (
            shaped_reward + 
            self.discount_factor * next_max_q - 
            current_q
        )
        
        self.q_table[state][action_idx] = new_q

#+end_src

And the ExplorationPotential class with more debug info:

#+begin_src python
class ExplorationPotential:
    def __init__(self, gamma=0.99, vicinity_size=2, debug=False):
        self.gamma = gamma
        self.vicinity_size = vicinity_size
        self.visited_coords = set()
        self.nearly_visited_coords = set()
        self.debug = debug
        
        # Track metrics
        self.total_potentials = []
        self

**** 

** Thompson Sampling

*** Explain thompson sampling relevance to this code / RL

This code currently uses Œµ-greedy exploration (random exploration with probability =exploration_rate=, else exploit best known action) rather than Thompson sampling.

Thompson sampling could be relevant here to improve exploration. Instead of just tracking Q-values, you could maintain a probability distribution over the expected rewards for each state-action pair. 

For example, you could:

1. Track mean and variance of rewards for each state-action
2. Model rewards as normal distributions
3. When selecting actions, sample from these distributions and pick highest sample

This would give you a more principled exploration strategy - actions with higher uncertainty would get sampled more frequently.

The key would be modifying =AIAgent.select_action()= to use Thompson sampling instead of Œµ-greedy. You'd need to track distributional parameters in addition to Q-values.

Would you like to see an example of how to implement Thompson sampling in this code?

*** Yes

Here's how we could modify the AIAgent class to implement Thompson sampling:

#+begin_src python
class AIAgent:
    def __init__(self, learning_rate=0.1, discount_factor=1.00):
        # For each state-action pair, track:
        self.q_means = defaultdict(lambda: np.zeros(len(Actions.list())))  # Mean rewards
        self.q_counts = defaultdict(lambda: np.zeros(len(Actions.list())))  # Number of samples 
        self.alpha = 1.0  # Prior alpha for Beta distribution
        self.beta = 1.0   # Prior beta for Beta distribution
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor

    def select_action(self, state):
        hack_action = self.run_from_battle_hack(state)
        if hack_action is not None:
            return hack_action.value

        possible_actions = Actions.list()
        
        # Sample from Beta distribution for each action
        samples = []
        for action_idx in range(len(possible_actions)):
            count = self.q_counts[state][action_idx]
            mean = self.q_means[state][action_idx]
            
            # Convert mean reward to success probability (between 0 and 1)
            success_prob = self.normalize_reward(mean)
            
            # Calculate Beta distribution parameters
            a = self.alpha + count * success_prob
            b = self.beta + count * (1 - success_prob)
            
            # Sample from Beta distribution
            sample = np.random.beta(a, b)
            samples.append(sample)

        # Choose action with highest sampled value
        action_index = np.argmax(samples)
        return possible_actions[action_index]

    def normalize_reward(self, reward):
        """Convert unbounded reward to [0,1] range for Beta distribution"""
        # You'll need to tune these parameters based on your reward scale
        max_expected_reward = 1000
        min_expected_reward = 0
        normalized = (reward - min_expected_reward) / (max_expected_reward - min_expected_reward)
        return np.clip(normalized, 0, 1)

    def update_q_table(self, state, action, next_state, reward):
        """Updates mean estimates using incremental update"""
        action_index = Actions.list().index(action)
        
        # Update count
        self.q_counts[state][action_index] += 1
        count = self.q_counts[state][action_index]

        # Update mean estimate
        current_mean = self.q_means[state][action_index]
        best_next_value = np.max(self.q_means[next_state])
        target = reward + self.discount_factor * best_next_value
        
        # Incremental update
        new_mean = current_mean + (target - current_mean) / count
        self.q_means[state][action_index] = new_mean
#+end_src

Key changes:

1. Instead of a single Q-table, we maintain:
   - =q_means=: Mean reward estimates 
   - =q_counts=: Number of times each state-action was tried

2. =select_action()= now uses Thompson sampling:
   - Converts mean rewards to [0,1] probabilities
   - Uses Beta distribution with parameters based on observed rewards
   - Samples from Beta distribution for each action
   - Picks action with highest sample

3. =update_q_table()= updates mean estimates incrementally

Advantages of this approach:

- More principled exploration - automatically balances exploration/exploitation
- Uncertainty in rewards is explicitly modeled
- No explicit exploration rate parameter needed
- Will focus exploration on promising but uncertain actions

You may need to tune:
- Prior parameters (alpha, beta)
- Reward normalization range
- Learning rate (though count-based updates may work better)

Would you like me to explain any part in more detail?

*** What are improvements to Thompson sampling?

Here are key improvements to Thompson sampling that could be applied to this RL context:

1. Particle Thompson Sampling
#+begin_src python
class AIAgent:
    def __init__(self, n_particles=10):
        self.n_particles = n_particles
        self.particles = defaultdict(
            lambda: [np.zeros(len(Actions.list())) for _ in range(n_particles)]
        )

    def select_action(self, state):
        # Sample one particle randomly
        particle_idx = np.random.randint(self.n_particles)
        particle = self.particles[state][particle_idx]
        return Actions.list()[np.argmax(particle)]
#+end_src

2. Bootstrapped Thompson Sampling
#+begin_src python
class AIAgent:
    def __init__(self, n_bootstrap=5):
        self.heads = [defaultdict(lambda: np.zeros(len(Actions.list()))) 
                     for _ in range(n_bootstrap)]
        self.masks = np.random.binomial(1, 0.5, n_bootstrap) 
        
    def update(self, state, action, reward):
        for i, head in enumerate(self.heads):
            if self.masks[i]:
                # Update only some heads randomly
                head[state][action] += reward
#+end_src

3. Information Directed Sampling (IDS)
#+begin_src python
def select_action(self, state):
    means = self.q_means[state]
    vars = self.q_vars[state] 
    
    # Calculate information gain
    info_gain = np.sqrt(vars)
    
    # Calculate regret
    best_mean = np.max(means)
    regret = best_mean - means
    
    # IDS ratio
    ratio = regret**2 / info_gain
    return Actions.list()[np.argmin(ratio)]
#+end_src

4. Gaussian Process Thompson Sampling
#+begin_src python
class AIAgent:
    def __init__(self):
        self.gp = GaussianProcessRegressor()
        self.X = []  # State-action pairs
        self.y = []  # Rewards

    def select_action(self, state):
        # Sample function from GP posterior
        f_sample = self.gp.sample_y(state)
        return Actions.list()[np.argmax(f_sample)]
#+end_src

5. Context-Aware Thompson Sampling
#+begin_src python
def select_action(self, state):
    context = self.extract_context(state)
    
    # Sample parameters for linear model
    theta = np.random.multivariate_normal(
        self.mu_posterior[state], 
        self.sigma_posterior[state]
    )
    
    # Predict rewards for each action
    predictions = [np.dot(theta, context) for _ in Actions.list()]
    return Actions.list()[np.argmax(predictions)]
#+end_src

The main improvements focus on:
- Better uncertainty estimation
- More efficient exploration
- Handling correlations between states/actions
- Adapting to non-stationary environments
- Incorporating domain knowledge/context

Would you like me to elaborate on any of these approaches?


*** Elaborate on each approach, no code just explain the motivation

Here's an explanation of each Thompson sampling improvement approach:

1. Particle Thompson Sampling
- Motivation: Better handles complex/multi-modal uncertainty distributions
- Instead of maintaining single estimate, keeps multiple "particles" representing different hypotheses
- More robust to wrong assumptions about reward distributions
- Can capture correlations and complex patterns that simple Beta/Gaussian can't
- Trade-off: Computational cost scales with particle count

2. Bootstrapped Thompson Sampling
- Motivation: More efficient exploration in deep RL settings
- Maintains ensemble of value functions trained on bootstrapped subsets of data
- Each "head" specializes in different regions of state space
- Provides diverse exploration strategies while being computationally efficient
- Particularly good for deep neural network value functions

3. Information Directed Sampling (IDS)
- Motivation: More principled exploration-exploitation trade-off
- Explicitly considers both expected regret and information gain
- Chooses actions that maximize information gain while minimizing regret
- More sample efficient than standard Thompson sampling
- Especially useful when exploration is expensive

4. Gaussian Process Thompson Sampling
- Motivation: Better handles continuous state/action spaces
- Models uncertainty using Gaussian processes
- Naturally handles correlations between similar states/actions
- Provides principled uncertainty estimates
- Good for smooth reward landscapes
- Limitation: Scales poorly with data size

5. Context-Aware Thompson Sampling
- Motivation: Incorporates domain knowledge and context
- Uses feature engineering to capture relevant state aspects
- Can transfer learning between similar contexts
- More efficient in high-dimensional spaces
- Especially useful when domain expertise available

The key theme across these improvements is handling the limitations of basic Thompson sampling:
- Better uncertainty estimation
- More efficient exploration
- Scaling to complex/continuous spaces
- Incorporating prior knowledge
- Computational efficiency

Would you like me to elaborate on any specific aspect?
